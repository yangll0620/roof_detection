{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30a6431-023e-45fa-a8d9-a3307c68bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "from six import BytesIO\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c56d7a5-e581-4bbc-b3f3-33d1e270417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41921c71-a801-473f-b910-63fbd6515413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import plot_detection, read_image_to_numpy\n",
    "from TFRecord_Generate_Parse import parse_tfexample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08afafb-098e-4eed-8919-725cbc84595e",
   "metadata": {},
   "source": [
    "## Prepare data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87389f91-8c32-4a3b-a4e2-e2042f377f4f",
   "metadata": {},
   "source": [
    "#### Convert to training/validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f36a45-076f-43d9-afae-389b5107669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_label_mappings = {'roof': 1}\n",
    "\n",
    "# By convention, our non-background classes start counting at 1.  \n",
    "category_index = {}\n",
    "for name, label in name_label_mappings.items():\n",
    "    category_index[label] = {'id': label, 'name': name}\n",
    "\n",
    "num_classes = len(name_label_mappings)\n",
    "\n",
    "# shifts all classes by a certain number of indices; so this so that the model receives one-hot labels where non-background classes start\n",
    "# counting at the zeroth index. This is ordinarily just handled automatically in our training binaries, but we need to reproduce it here.\n",
    "label_id_offset = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea3a8d1-aec8-4076-8078-7cc8a938e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_tfrecordFile(tfrecord_file):\n",
    "    \"\"\" load data from tfrecord_file\n",
    "        Args:\n",
    "            tfrecord_file:\n",
    "        \n",
    "        Return:\n",
    "            images_np\n",
    "            gt_boxes\n",
    "            class_names\n",
    "            class_labels\n",
    "            filenames\n",
    "\n",
    "    \"\"\"\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
    "    parsed_dataset = dataset.map(parse_tfexample)\n",
    "\n",
    "\n",
    "    # extract from parsed_dataset  \n",
    "    gt_boxes = [] \n",
    "    images_np = [] \n",
    "    class_names = []\n",
    "    class_labels = []\n",
    "    filenames = []\n",
    "    for image, height, width, filename, boxes, class_name, class_label in parsed_dataset:\n",
    "        filenames.append(filename.numpy().decode('utf-8'))\n",
    "\n",
    "        images_np.append(image.numpy()) \n",
    "\n",
    "        gt_boxes.append(boxes.numpy().T)\n",
    "        class_names.append([name.decode() for name in class_name.numpy()])\n",
    "        class_labels.append(class_label.numpy())\n",
    "\n",
    "    return images_np, gt_boxes, class_names, class_labels, filenames\n",
    "\n",
    "\n",
    "def convert_2_tensor(images_np, gt_boxes, gt_class_labels, label_id_offset, num_classes): \n",
    "    \"\"\" convert to dataset used for training from tfrecord_file\n",
    "        Args:\n",
    "            tfrecord_file:\n",
    "        \n",
    "        Return:\n",
    "            image_tensors\n",
    "            gt_box_tensors\n",
    "            gt_classes_one_hot_tensors\n",
    "    \"\"\"\n",
    "\n",
    "    # convert everythin to tensors, convert class labels to one-hot\n",
    "    image_tensors = []\n",
    "    gt_box_tensors = []\n",
    "    gt_classes_one_hot_tensors = []\n",
    "    for image_np, gt_box, gt_class_label in zip(images_np, gt_boxes, gt_class_labels):\n",
    "        image_tensors.append(tf.convert_to_tensor(image_np, dtype=tf.float32))\n",
    "        gt_box_tensors.append(tf.convert_to_tensor(gt_box, dtype=tf.float32))\n",
    "\n",
    "        zero_indexed_groundtruth_classes = tf.convert_to_tensor(gt_class_label - label_id_offset)\n",
    "        gt_classes_one_hot_tensors.append(tf.one_hot(zero_indexed_groundtruth_classes, num_classes))\n",
    "    \n",
    "\n",
    "    return image_tensors, gt_box_tensors, gt_classes_one_hot_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a00ac90-3777-4ee2-b1e8-45a9d317cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load training dataset\n",
    "label_id_offset = 1\n",
    "\n",
    "train_images_np, train_gt_boxes, train_class_names, train_class_labels, train_filenames = load_from_tfrecordFile('data/sample_data/roof/train.tfrecord')\n",
    "train_image_tensors, train_gt_box_tensors, train_gt_classes_one_hot_tensors = convert_2_tensor(train_images_np, train_gt_boxes, train_class_labels, label_id_offset, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0d8e7-efe9-4d46-b5c4-d017f9c171dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load validation dataset\n",
    "valid_images_np, valid_gt_boxes, valid_class_names, valid_class_labels, valid_filenames = load_from_tfrecordFile('data/sample_data/roof/valid.tfrecord')\n",
    "valid_image_tensors, valid_gt_box_tensors, valid_gt_classes_one_hot_tensors = convert_2_tensor(valid_images_np, valid_gt_boxes, valid_class_labels, label_id_offset, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5666c3-0269-4924-a97a-2c6b559bc86d",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20848097-981d-4b7d-8e84-f363a23f2eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = False\n",
    "if visualize:\n",
    "    # select one file \n",
    "    idx = 0\n",
    "    \n",
    "    print(f\"visulize file {train_filenames[idx]}\")\n",
    "    \n",
    "    image_np = train_images_np[idx]\n",
    "    boxes = train_gt_boxes[idx]\n",
    "    classes = train_class_labels[idx]\n",
    "    scores = np.ones(shape=boxes.shape[0], dtype=np.float32)\n",
    "    \n",
    "    plt.figure(figsize=(30,15))\n",
    "    plt.subplot(2, 3, 1)\n",
    "    \n",
    "    plot_detection(image_np, boxes, classes, scores = scores, category_index = category_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9096f7-3edb-46f9-9b49-f570e3363cb9",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "150b679c-27f5-44f0-933f-2cc32f0d3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detection_model(pipeline_config_path, num_classes = 1):\n",
    "    \"\"\" Create a detection model based on pipeline_config.\n",
    "\n",
    "        Args:\n",
    "            pipeline_config_path: Path to pipeline_pb2.TrainEvalPipelineConfig text proto.\n",
    "    \"\"\"\n",
    "\n",
    "    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)\n",
    "    model_config = configs['model']\n",
    "    model_config.ssd.num_classes = num_classes\n",
    "    model_config.ssd.freeze_batchnorm = True\n",
    "    detection_model = model_builder.build(model_config=model_config, is_training=True)\n",
    "\n",
    "    return detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6f048f3-84e0-4eb9-b009-3539a1aac29a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pipeline_config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m detection_model \u001b[38;5;241m=\u001b[39m create_detection_model(pipeline_config_path, num_classes \u001b[38;5;241m=\u001b[39m num_classes)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_classes' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline_config_path = 'object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
    "detection_model = create_detection_model(pipeline_config_path, num_classes = num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20ab648-1a1d-467b-b738-343bb457ab50",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd574151-376b-40d0-95f1-f945242a0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.keras.backend.set_learning_phase(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad297740-3105-4d69-abe5-102978650415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These parameters can be tuned\n",
    "batch_size = 20\n",
    "learning_rate = 0.01\n",
    "num_batches = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9af952-d5af-43a4-a77c-22500319e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate = learning_rate, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc464aca-78c8-4ae7-b4df-9b227e52bdf1",
   "metadata": {},
   "source": [
    "### Restore weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608f138e-efde-48d9-a1e9-55368bc4840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare CheckpointManager for checkpoints\n",
    "checkpoint_dir = 'models/finetuned_models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8'\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, dtype=tf.compat.v2.dtypes.int64, name='global_step',\n",
    "        aggregation=tf.compat.v2.VariableAggregation.ONLY_FIRST_REPLICA)\n",
    "ckpt = tf.compat.v2.train.Checkpoint(step=global_step, model=detection_model, optimizer=optimizer)\n",
    "\n",
    "manager = tf.compat.v2.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a186107d-3b19-4b4c-9d4c-acbbc1b20e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_weights_from_pretrained(detection_model, pretrained_checkpoint_path):\n",
    "    \"\"\" Restore weights from pretrained model\n",
    "\n",
    "        Args:\n",
    "            detection_model: detection model\n",
    "            pretrain_checkpoint_path: pretrained model checkpoint path\n",
    "\n",
    "        Returns:\n",
    "            detection_model: detection_model with weights restored from pretrained model specified by checkpoint_path\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up object-based checkpoint restore --- RetinaNet has two prediction\n",
    "    # `heads` --- one for classification, the other for box regression.  \n",
    "    # restore the box regression head but initialize the classification head from scratch \n",
    "    fake_box_predictor = tf.compat.v2.train.Checkpoint(\n",
    "        _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
    "        _box_prediction_head=detection_model._box_predictor._box_prediction_head)\n",
    "    \n",
    "    fake_model = tf.compat.v2.train.Checkpoint(_feature_extractor=detection_model._feature_extractor,\n",
    "                                           _box_predictor=fake_box_predictor)\n",
    "    ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)\n",
    "    \n",
    "    ckpt.restore(pretrained_checkpoint_path).expect_partial()\n",
    "\n",
    "\n",
    "    # Run model through a dummy image so that variables are created\n",
    "    image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    _ = detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "    print('Weights of detection_model restored from \\n{}'.format(pretrained_checkpoint_path))\n",
    "\n",
    "    return detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f2576-b373-4199-8f91-fe60253abd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "FromPretrained = False\n",
    "\n",
    "if not FromPretrained and latest_checkpoint:\n",
    "    ckpt.restore(latest_checkpoint)\n",
    "    print(\"The weights of detection_model restored from : \\n{}\".format(latest_checkpoint))\n",
    "    \n",
    "else:\n",
    "    pretrained_checkpoint_path = 'models/pretrained_models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0'\n",
    "    detection_model =restore_weights_from_pretrained(detection_model, pretrained_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c62dc-ebf7-49e3-ab9b-5228a6e1aed6",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1bd9ae-b9b6-405f-a6de-2f6b2ef9bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables in top layers to fine-tune.\n",
    "trainable_variables = detection_model.trainable_variables\n",
    "to_fine_tune = []\n",
    "prefixes_to_train = ['WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead', \n",
    "                    'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
    "\n",
    "for var in trainable_variables:\n",
    "    if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
    "        to_fine_tune.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d64ad18-f5ac-4746-9357-22c5422884aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
    "    \"\"\"Get a tf.function for training step.\"\"\"\n",
    "    \n",
    "    def train_step_fn(image_tensors, gt_boxes_list, gt_classes_list):\n",
    "        \"\"\" A single training iteration\n",
    "\n",
    "            Args:\n",
    "                image_tensors:  a list of [height_in, width_in, channels] float tensor, len = batch_size \n",
    "                gt_boxes_list:\n",
    "                gt_classes_list:\n",
    "        \"\"\"\n",
    "    \n",
    "        shapes = tf.constant(batch_size * [[640, 640, 3]], dtype=tf.int32)\n",
    "        model.provide_groundtruth(groundtruth_boxes_list = gt_boxes_list, groundtruth_classes_list =gt_classes_list)\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            preprocessed_images = tf.concat([model.preprocess(tf.expand_dims(image_tensor, axis = 0))[0] for image_tensor in image_tensors],axis = 0)\n",
    "            prediction_dict = model.predict(preprocessed_images, shapes)\n",
    "            losses_dict = model.loss(prediction_dict, shapes)\n",
    "            total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
    "    \n",
    "            gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
    "            optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
    "            \n",
    "        return total_loss\n",
    "\n",
    "    \n",
    "    def valid_step_fn(image_tensors, gt_boxes_list, gt_classes_list):\n",
    "        \"\"\" validate model \n",
    "\n",
    "            Args:\n",
    "                image_tensors:  a list of [height_in, width_in, channels] float tensor, len = batch_size \n",
    "                gt_boxes_list:\n",
    "                gt_classes_list:\n",
    "        \"\"\"\n",
    "        shapes = tf.constant(batch_size * [[640, 640, 3]], dtype=tf.int32)\n",
    "        model.provide_groundtruth(groundtruth_boxes_list = gt_boxes_list, groundtruth_classes_list =gt_classes_list)\n",
    "\n",
    "\n",
    "        preprocessed_images = tf.concat([model.preprocess(tf.expand_dims(image_tensor, axis = 0))[0] for image_tensor in image_tensors],axis = 0)\n",
    "        prediction_dict = model.predict(preprocessed_images, shapes)\n",
    "        losses_dict = model.loss(prediction_dict, shapes)\n",
    "        total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
    "\n",
    "\n",
    "        return total_loss\n",
    "        \n",
    "\n",
    "    return train_step_fn, valid_step_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ca288-9d38-4ab2-8b0b-7024ea16008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_fn, valid_step_fn = get_model_train_step_function(detection_model, optimizer, to_fine_tune)\n",
    "\n",
    "print('Start fine-tuning!', flush=True)\n",
    "for idx in range(num_batches):\n",
    "\n",
    "    # Grab a random subset of examples\n",
    "    all_indices = list(range(len(train_images_np)))\n",
    "    random.shuffle(all_indices)\n",
    "    batch_indices = all_indices[0:batch_size]\n",
    "\n",
    "    train_gt_boxes_list = [train_gt_box_tensors[idx] for idx in batch_indices]\n",
    "    train_gt_classes_list = [train_gt_classes_one_hot_tensors[idx] for idx in batch_indices]\n",
    "    train_image_tensors = [train_image_tensors[idx] for idx in batch_indices]\n",
    "\n",
    "    # training\n",
    "    total_trainingloss = train_step_fn(train_image_tensors, train_gt_boxes_list, train_gt_classes_list)\n",
    "\n",
    "    if idx % 10 == 0 or idx == num_batches-1:\n",
    "        \n",
    "        # perform on validation files\n",
    "        total_validateloss = valid_step_fn(valid_image_tensors, valid_gt_box_tensors, valid_gt_classes_one_hot_tensors)\n",
    "\n",
    "        # print the training and validation loss\n",
    "        print('batch ' + str(idx+1) + ' of ' + str(num_batches) + \n",
    "              ',training loss=' +  str(total_trainingloss.numpy()) +\n",
    "              ', validation loss=' +  str(total_validateloss.numpy()), \n",
    "              flush=True)\n",
    "\n",
    "    # save every 100 steps\n",
    "    if idx % 100 == 0 or idx == num_batches-1:\n",
    "        print(\"saved one checkpoint at  \" + manager.save())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0873bf4-dcd4-43ba-a723-ee088a2e0c44",
   "metadata": {},
   "source": [
    "## Load test images and run inference with new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3a882d-e1fb-4ed1-86b9-1cc242ef4621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_detect(model, input_tensor):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            input_tensor: [height, width, 3]\n",
    "    \"\"\"\n",
    "    preprocessed_images, shapes = model.preprocess(tf.expand_dims(input_tensor, axis=0))\n",
    "    prediction_dict = model.predict(preprocessed_images, shapes)\n",
    "    postprocessed_dict = model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "    return postprocessed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "904957da-adf6-455a-96af-6b3c8610fa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of detection_model restored from : \n",
      "models/finetuned_models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/ckpt-2\n"
     ]
    }
   ],
   "source": [
    "load_latest_checkpoint = True\n",
    "if load_latest_checkpoint:\n",
    "\n",
    "    pipeline_config_path = 'object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
    "    detection_model = create_detection_model(pipeline_config_path, num_classes = num_classes)\n",
    "    \n",
    "    checkpoint_dir = 'models/finetuned_models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8'\n",
    "    \n",
    "    ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    ckpt.restore(latest_checkpoint)\n",
    "    print(\"The weights of detection_model restored from : \\n{}\".format(latest_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24274983-124e-4c00-86d2-29a543064187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected image saved in data/sample_data/roof/rawdata/test/detected_prediction_image_1-2.jpg\n",
      "Detected image saved in data/sample_data/roof/rawdata/test/detected_prediction_image_1-1.jpg\n"
     ]
    }
   ],
   "source": [
    "image_format = '.tif'\n",
    "test_image_dir = 'data/sample_data/roof/rawdata/test'\n",
    "test_filenames = glob.glob(test_image_dir + '/*' + image_format)\n",
    "\n",
    "detected_img_format = '.jpg'\n",
    "\n",
    "for image_file in test_filenames:\n",
    "    image_np = read_image_to_numpy(image_file)\n",
    "    input_tensor = tf.convert_to_tensor(image_np, dtype=tf.float32)\n",
    "\n",
    "    detections = obj_detect(model = detection_model, input_tensor=input_tensor)\n",
    "    \n",
    "    filename = os.path.basename(image_file)\n",
    "    filename_prefix = filename.split('.tif')[0]\n",
    "    filename_prefix = filename_prefix.replace(' ', '_').replace('.', '-')\n",
    "    detected_file = os.path.join(test_image_dir, 'detected_'+ filename_prefix + detected_img_format)\n",
    "    print(\"Detected image saved in {}\".format(detected_file))\n",
    "    plot_detection(image_np,\n",
    "                   detections['detection_boxes'][0].numpy(), \n",
    "                   detections['detection_classes'][0].numpy().astype(np.int32) + label_id_offset,\n",
    "                   detections['detection_scores'][0].numpy(),\n",
    "                   category_index, \n",
    "                   min_score_thresh = 0.3,\n",
    "                   figsize=(15, 20), \n",
    "                   image_name=detected_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
